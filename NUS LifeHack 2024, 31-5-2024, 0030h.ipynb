{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9d02737",
   "metadata": {},
   "source": [
    "# Scam Detection Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67bb3613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd8e637",
   "metadata": {},
   "source": [
    "### Generating a Toy Dataset for Scams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bb6b177",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faker in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (25.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.4 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from faker) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.4->faker) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db2b5c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toy dataset saved to toy_dataset_with_probabilities.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "from faker import Faker\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Define number of records\n",
    "num_records = 10000\n",
    "\n",
    "# Helper functions for generating data\n",
    "def generate_transaction_amount(is_scam):\n",
    "    if is_scam:\n",
    "        return np.round(np.random.uniform(50, 5000), 2)  # Larger amounts for scams\n",
    "    else:\n",
    "        return np.round(np.random.uniform(10, 1000), 2)  # Smaller amounts for normal transactions\n",
    "\n",
    "def generate_transaction_datetime():\n",
    "    # Simulate more transactions during weekdays and less on weekends\n",
    "    weekday_prob = 0.8\n",
    "    if random.random() < weekday_prob:\n",
    "        # Random weekday time\n",
    "        day = np.random.randint(0, 5)  # Monday to Friday\n",
    "        hour = np.random.randint(8, 18)  # Business hours\n",
    "    else:\n",
    "        # Random weekend time\n",
    "        day = np.random.randint(5, 7)  # Saturday and Sunday\n",
    "        hour = np.random.randint(8, 20)  # Wider range on weekends\n",
    "    minute = np.random.randint(0, 60)\n",
    "    second = np.random.randint(0, 60)\n",
    "    return datetime(2023, 1, 1) + timedelta(days=day, hours=hour, \n",
    "                                            minutes=minute, seconds=second)\n",
    "\n",
    "def generate_is_scam_prob():\n",
    "    # Generate a probability for a transaction to be a scam\n",
    "    return np.random.uniform(0, 1)\n",
    "\n",
    "# Function to generate scam names based on certain patterns\n",
    "def generate_scam_name():\n",
    "    return fake.first_name() + \" \" + fake.last_name_nonbinary()\n",
    "\n",
    "# Function to generate non-scam names based on common naming conventions\n",
    "def generate_non_scam_name():\n",
    "    return fake.first_name() + \" \" + fake.last_name()\n",
    "\n",
    "# Function to generate names based on scam status\n",
    "def generate_name(is_scam):\n",
    "    if is_scam:\n",
    "        return generate_scam_name()\n",
    "    else:\n",
    "        return generate_non_scam_name()\n",
    "\n",
    "# Function to generate scam messages\n",
    "def generate_scam_message():\n",
    "    return fake.sentence(nb_words=6) + \" Urgent action required!\"\n",
    "\n",
    "# Function to generate non-scam messages\n",
    "def generate_non_scam_message():\n",
    "    return \"Thank you for your recent transaction. Your account balance is updated.\"\n",
    "\n",
    "# Function to generate messages based on scam status\n",
    "def generate_message(is_scam):\n",
    "    if is_scam:\n",
    "        return generate_scam_message()\n",
    "    else:\n",
    "        return generate_non_scam_message()\n",
    "\n",
    "# Function to determine the scam possibility based on the probability\n",
    "def determine_scam_possibility(prob):\n",
    "    if 0.9 < prob < 1:\n",
    "        return 'Highest'\n",
    "    elif 0.5 < prob < 0.8:\n",
    "        return 'Moderate'\n",
    "    elif prob <= 0.5:\n",
    "        return 'Low'\n",
    "    else:\n",
    "        return 'Lowest'\n",
    "\n",
    "# Generate data\n",
    "data = []\n",
    "for _ in range(num_records):\n",
    "    is_scam_prob = generate_is_scam_prob()\n",
    "    transaction_amount = generate_transaction_amount(is_scam_prob > 0.5)\n",
    "    transaction_datetime = generate_transaction_datetime()\n",
    "    recipient_name = generate_name(is_scam_prob > 0.5)\n",
    "    sender_name = generate_name(is_scam_prob > 0.5)\n",
    "    message = generate_message(is_scam_prob > 0.5)\n",
    "    scam_possibility = determine_scam_possibility(is_scam_prob)\n",
    "    data.append({\n",
    "        'transaction_amount': transaction_amount,\n",
    "        'transaction_datetime': transaction_datetime,\n",
    "        'is_scam_prob': is_scam_prob,\n",
    "        'scam_possibility': scam_possibility,\n",
    "        'recipient_name': recipient_name,\n",
    "        'sender_name': sender_name,\n",
    "        'message': message\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the toy dataset to CSV\n",
    "toy_dataset_path = 'toy_dataset_with_probabilities.csv'\n",
    "df.to_csv(toy_dataset_path, index=False)\n",
    "\n",
    "print(f\"Toy dataset saved to {toy_dataset_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "282cbb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "36ec6ef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_amount</th>\n",
       "      <th>transaction_datetime</th>\n",
       "      <th>is_scam_prob</th>\n",
       "      <th>scam_possibility</th>\n",
       "      <th>recipient_name</th>\n",
       "      <th>sender_name</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>951.21</td>\n",
       "      <td>2023-01-03 15:20:38</td>\n",
       "      <td>0.374540</td>\n",
       "      <td>Low</td>\n",
       "      <td>Michele Davenport</td>\n",
       "      <td>Victoria Bennett</td>\n",
       "      <td>Thank you for your recent transaction. Your ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>108.98</td>\n",
       "      <td>2023-01-03 15:52:35</td>\n",
       "      <td>0.445833</td>\n",
       "      <td>Low</td>\n",
       "      <td>Courtney Alvarez</td>\n",
       "      <td>Kara Morrow</td>\n",
       "      <td>Thank you for your recent transaction. Your ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>654.38</td>\n",
       "      <td>2023-01-05 09:23:43</td>\n",
       "      <td>0.142867</td>\n",
       "      <td>Low</td>\n",
       "      <td>Anthony Hardin</td>\n",
       "      <td>William Reed</td>\n",
       "      <td>Thank you for your recent transaction. Your ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53.85</td>\n",
       "      <td>2023-01-04 12:32:11</td>\n",
       "      <td>0.938553</td>\n",
       "      <td>Highest</td>\n",
       "      <td>Joanna Shepherd</td>\n",
       "      <td>Thomas West</td>\n",
       "      <td>Center meeting million machine. Urgent action ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84.98</td>\n",
       "      <td>2023-01-01 08:26:58</td>\n",
       "      <td>0.611653</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>Justin Guerra</td>\n",
       "      <td>Micheal Harris</td>\n",
       "      <td>Case meet improve know dog cost down. Urgent a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   transaction_amount transaction_datetime  is_scam_prob scam_possibility  \\\n",
       "0              951.21  2023-01-03 15:20:38      0.374540              Low   \n",
       "1              108.98  2023-01-03 15:52:35      0.445833              Low   \n",
       "2              654.38  2023-01-05 09:23:43      0.142867              Low   \n",
       "3               53.85  2023-01-04 12:32:11      0.938553          Highest   \n",
       "4               84.98  2023-01-01 08:26:58      0.611653         Moderate   \n",
       "\n",
       "      recipient_name       sender_name  \\\n",
       "0  Michele Davenport  Victoria Bennett   \n",
       "1   Courtney Alvarez       Kara Morrow   \n",
       "2     Anthony Hardin      William Reed   \n",
       "3    Joanna Shepherd       Thomas West   \n",
       "4      Justin Guerra    Micheal Harris   \n",
       "\n",
       "                                             message  \n",
       "0  Thank you for your recent transaction. Your ac...  \n",
       "1  Thank you for your recent transaction. Your ac...  \n",
       "2  Thank you for your recent transaction. Your ac...  \n",
       "3  Center meeting million machine. Urgent action ...  \n",
       "4  Case meet improve know dog cost down. Urgent a...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7048f893",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Preprocess the text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    tokens = word_tokenize(text)  # Tokenize\n",
    "    tokens = [word for word in tokens if word.isalpha()]  # Remove non-alphabetic tokens\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing to the text columns\n",
    "data['message_tokens'] = data['message'].apply(preprocess_text)\n",
    "data['recipient_name_tokens'] = data['recipient_name'].apply(preprocess_text)\n",
    "data['sender_name_tokens'] = data['sender_name'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19036807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Word2Vec model on the tokens\n",
    "all_tokens = data['message_tokens'].tolist() + data['recipient_name_tokens'].tolist() + data['sender_name_tokens'].tolist()\n",
    "word2vec_model = gensim.models.Word2Vec(all_tokens, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Function to get the embedding for a text\n",
    "def get_text_embedding(tokens, model):\n",
    "    embedding = np.mean([model.wv[word] for word in tokens if word in model.wv], axis=0)\n",
    "    if isinstance(embedding, np.ndarray):\n",
    "        return embedding\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "# Get embeddings for the text columns\n",
    "data['message_embedding'] = data['message_tokens'].apply(lambda x: get_text_embedding(x, word2vec_model).tolist())\n",
    "data['recipient_name_embedding'] = data['recipient_name_tokens'].apply(lambda x: get_text_embedding(x, word2vec_model).tolist())\n",
    "data['sender_name_embedding'] = data['sender_name_tokens'].apply(lambda x: get_text_embedding(x, word2vec_model).tolist())\n",
    "\n",
    "# Drop the token columns\n",
    "data = data.drop(columns=['message_tokens', 'recipient_name_tokens', 'sender_name_tokens'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1138bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert embedding columns from lists to arrays of floats\n",
    "data['message_embedding'] = data['message_embedding'].apply(lambda x: np.array(x, dtype=np.float32))\n",
    "data['recipient_name_embedding'] = data['recipient_name_embedding'].apply(lambda x: np.array(x, dtype=np.float32))\n",
    "data['sender_name_embedding'] = data['sender_name_embedding'].apply(lambda x: np.array(x, dtype=np.float32))\n",
    "\n",
    "# Expand arrays into separate columns\n",
    "message_embedding_data = pd.DataFrame(data['message_embedding'].tolist(), index=data.index).add_prefix('message_embedding_')\n",
    "recipient_name_embedding_data = pd.DataFrame(data['recipient_name_embedding'].tolist(), index=data.index).add_prefix('recipient_name_embedding_')\n",
    "sender_name_embedding_data = pd.DataFrame(data['sender_name_embedding'].tolist(), index=data.index).add_prefix('sender_name_embedding_')\n",
    "\n",
    "# Concatenate embeddings into the original dataframe\n",
    "data = pd.concat([data, message_embedding_data, recipient_name_embedding_data, sender_name_embedding_data], axis=1)\n",
    "\n",
    "# Drop the original embedding columns\n",
    "data = data.drop(columns=['message_embedding', 'recipient_name_embedding', 'sender_name_embedding'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6fe53454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Columns: 307 entries, transaction_amount to sender_name_embedding_99\n",
      "dtypes: datetime64[ns](1), float32(300), float64(2), object(4)\n",
      "memory usage: 12.0+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ff6f7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Columns: 302 entries, transaction_amount to sender_name_embedding_99\n",
      "dtypes: float32(300), float64(2)\n",
      "memory usage: 11.6 MB\n"
     ]
    }
   ],
   "source": [
    "# Identify columns with object and datetime64[ns] data types\n",
    "columns_to_drop = data.select_dtypes(include=['object', 'datetime64[ns]']).columns\n",
    "\n",
    "# Drop the identified columns\n",
    "data = data.drop(columns=columns_to_drop)\n",
    "\n",
    "data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d5d1f2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['is_scam'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report, accuracy_score\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Prepare features and target variable\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mis_scam\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m y \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_scam\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Split the data into training and testing sets\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:5581\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5434\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5435\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5442\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5443\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5445\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5446\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5579\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5583\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5587\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5588\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5589\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:4788\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4788\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:4830\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4828\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4829\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4830\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4831\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4833\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4834\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['is_scam'] not found in axis\""
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Prepare features and target variable\n",
    "X = data.drop(columns=['is_scam'])\n",
    "y = data['is_scam']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Classification Report:\\n{report}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde8acb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
