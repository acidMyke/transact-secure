{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9d02737",
   "metadata": {},
   "source": [
    "# Scam Detection Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67bb3613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd8e637",
   "metadata": {},
   "source": [
    "### Generating a Toy Dataset for Scams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bb6b177",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faker in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (25.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.4 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from faker) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.4->faker) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db2b5c82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sender</th>\n",
       "      <th>Recipient</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Transaction_Frequency</th>\n",
       "      <th>Is_Scam</th>\n",
       "      <th>Day</th>\n",
       "      <th>Month</th>\n",
       "      <th>Year</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Minute</th>\n",
       "      <th>Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Charlie</td>\n",
       "      <td>David</td>\n",
       "      <td>1560</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>2024</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Charlie</td>\n",
       "      <td>Harry</td>\n",
       "      <td>4278</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>2023</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>John</td>\n",
       "      <td>Charlie</td>\n",
       "      <td>7345</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>John</td>\n",
       "      <td>David</td>\n",
       "      <td>2148</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>2024</td>\n",
       "      <td>22</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alice</td>\n",
       "      <td>Charlie</td>\n",
       "      <td>816</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sender Recipient  Amount  Transaction_Frequency  Is_Scam  Day  Month  \\\n",
       "0  Charlie     David    1560                      1        0   25      2   \n",
       "1  Charlie     Harry    4278                      2        0    5     12   \n",
       "2     John   Charlie    7345                      1        0   27      2   \n",
       "3     John     David    2148                      2        0    7      4   \n",
       "4    Alice   Charlie     816                      1        0    1      2   \n",
       "\n",
       "   Year  Hour  Minute  Second  \n",
       "0  2024    14       6      10  \n",
       "1  2023     8      10      44  \n",
       "2  2024     1      26      56  \n",
       "3  2024    22      12      13  \n",
       "4  2024     1      32      34  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import pickle\n",
    "\n",
    "# Sample data generation\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generating 1000 records\n",
    "num_records = 1000\n",
    "names = ['John', 'Harry', 'Alice', 'Bob', 'Charlie', 'David']\n",
    "transactions = []\n",
    "\n",
    "# Generate timestamps\n",
    "start_date = datetime.now() - timedelta(days=365)\n",
    "timestamps = [start_date + timedelta(seconds=int(x)) for x in np.random.randint(0, 365*24*3600, num_records)]\n",
    "\n",
    "# Initialize a dictionary to keep track of transaction frequency per user\n",
    "transaction_count = {name: 0 for name in names}\n",
    "\n",
    "for i in range(num_records):\n",
    "    sender = np.random.choice(names)\n",
    "    recipient = np.random.choice(names)\n",
    "    while recipient == sender:\n",
    "        recipient = np.random.choice(names)\n",
    "    \n",
    "    amount = np.random.randint(100, 10000)\n",
    "    transaction_frequency = transaction_count[sender] + 1  # Increment transaction frequency for sender\n",
    "    is_scam = 0\n",
    "\n",
    "    # More realistic scam flagging\n",
    "    if sender in ['John', 'Harry']:\n",
    "        if sender == 'John' and transaction_frequency == 7 and recipient in ['Harry', 'Alice']:\n",
    "            is_scam = 1\n",
    "        elif sender == 'Harry' and transaction_frequency == 7 and recipient in ['John', 'Bob']:\n",
    "            is_scam = 1\n",
    "    else:\n",
    "        if transaction_frequency > 10 and amount > 5000:\n",
    "            is_scam = 1\n",
    "\n",
    "    transactions.append([timestamps[i], sender, recipient, amount, transaction_frequency, is_scam])\n",
    "    \n",
    "    # Update transaction count\n",
    "    transaction_count[sender] += 1\n",
    "\n",
    "# Create a DataFrame\n",
    "columns = ['Timestamp', 'Sender', 'Recipient', 'Amount', 'Transaction_Frequency', 'Is_Scam']\n",
    "df = pd.DataFrame(transactions, columns=columns)\n",
    "\n",
    "# Feature engineering on timestamp\n",
    "df['Day'] = df['Timestamp'].dt.day\n",
    "df['Month'] = df['Timestamp'].dt.month\n",
    "df['Year'] = df['Timestamp'].dt.year\n",
    "df['Hour'] = df['Timestamp'].dt.hour\n",
    "df['Minute'] = df['Timestamp'].dt.minute\n",
    "df['Second'] = df['Timestamp'].dt.second\n",
    "\n",
    "# Drop the Timestamp column\n",
    "df = df.drop(columns=['Timestamp'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "282cbb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "36ec6ef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_amount</th>\n",
       "      <th>transaction_datetime</th>\n",
       "      <th>is_scam_prob</th>\n",
       "      <th>scam_possibility</th>\n",
       "      <th>recipient_name</th>\n",
       "      <th>sender_name</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>951.21</td>\n",
       "      <td>2023-01-03 15:20:38</td>\n",
       "      <td>0.374540</td>\n",
       "      <td>Low</td>\n",
       "      <td>Michele Davenport</td>\n",
       "      <td>Victoria Bennett</td>\n",
       "      <td>Thank you for your recent transaction. Your ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>108.98</td>\n",
       "      <td>2023-01-03 15:52:35</td>\n",
       "      <td>0.445833</td>\n",
       "      <td>Low</td>\n",
       "      <td>Courtney Alvarez</td>\n",
       "      <td>Kara Morrow</td>\n",
       "      <td>Thank you for your recent transaction. Your ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>654.38</td>\n",
       "      <td>2023-01-05 09:23:43</td>\n",
       "      <td>0.142867</td>\n",
       "      <td>Low</td>\n",
       "      <td>Anthony Hardin</td>\n",
       "      <td>William Reed</td>\n",
       "      <td>Thank you for your recent transaction. Your ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53.85</td>\n",
       "      <td>2023-01-04 12:32:11</td>\n",
       "      <td>0.938553</td>\n",
       "      <td>Highest</td>\n",
       "      <td>Joanna Shepherd</td>\n",
       "      <td>Thomas West</td>\n",
       "      <td>Center meeting million machine. Urgent action ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84.98</td>\n",
       "      <td>2023-01-01 08:26:58</td>\n",
       "      <td>0.611653</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>Justin Guerra</td>\n",
       "      <td>Micheal Harris</td>\n",
       "      <td>Case meet improve know dog cost down. Urgent a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   transaction_amount transaction_datetime  is_scam_prob scam_possibility  \\\n",
       "0              951.21  2023-01-03 15:20:38      0.374540              Low   \n",
       "1              108.98  2023-01-03 15:52:35      0.445833              Low   \n",
       "2              654.38  2023-01-05 09:23:43      0.142867              Low   \n",
       "3               53.85  2023-01-04 12:32:11      0.938553          Highest   \n",
       "4               84.98  2023-01-01 08:26:58      0.611653         Moderate   \n",
       "\n",
       "      recipient_name       sender_name  \\\n",
       "0  Michele Davenport  Victoria Bennett   \n",
       "1   Courtney Alvarez       Kara Morrow   \n",
       "2     Anthony Hardin      William Reed   \n",
       "3    Joanna Shepherd       Thomas West   \n",
       "4      Justin Guerra    Micheal Harris   \n",
       "\n",
       "                                             message  \n",
       "0  Thank you for your recent transaction. Your ac...  \n",
       "1  Thank you for your recent transaction. Your ac...  \n",
       "2  Thank you for your recent transaction. Your ac...  \n",
       "3  Center meeting million machine. Urgent action ...  \n",
       "4  Case meet improve know dog cost down. Urgent a...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7048f893",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Preprocess the text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    tokens = word_tokenize(text)  # Tokenize\n",
    "    tokens = [word for word in tokens if word.isalpha()]  # Remove non-alphabetic tokens\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing to the text columns\n",
    "data['message_tokens'] = data['message'].apply(preprocess_text)\n",
    "data['recipient_name_tokens'] = data['recipient_name'].apply(preprocess_text)\n",
    "data['sender_name_tokens'] = data['sender_name'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19036807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Word2Vec model on the tokens\n",
    "all_tokens = data['message_tokens'].tolist() + data['recipient_name_tokens'].tolist() + data['sender_name_tokens'].tolist()\n",
    "word2vec_model = gensim.models.Word2Vec(all_tokens, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Function to get the embedding for a text\n",
    "def get_text_embedding(tokens, model):\n",
    "    embedding = np.mean([model.wv[word] for word in tokens if word in model.wv], axis=0)\n",
    "    if isinstance(embedding, np.ndarray):\n",
    "        return embedding\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "# Get embeddings for the text columns\n",
    "data['message_embedding'] = data['message_tokens'].apply(lambda x: get_text_embedding(x, word2vec_model).tolist())\n",
    "data['recipient_name_embedding'] = data['recipient_name_tokens'].apply(lambda x: get_text_embedding(x, word2vec_model).tolist())\n",
    "data['sender_name_embedding'] = data['sender_name_tokens'].apply(lambda x: get_text_embedding(x, word2vec_model).tolist())\n",
    "\n",
    "# Drop the token columns\n",
    "data = data.drop(columns=['message_tokens', 'recipient_name_tokens', 'sender_name_tokens'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1138bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert embedding columns from lists to arrays of floats\n",
    "data['message_embedding'] = data['message_embedding'].apply(lambda x: np.array(x, dtype=np.float32))\n",
    "data['recipient_name_embedding'] = data['recipient_name_embedding'].apply(lambda x: np.array(x, dtype=np.float32))\n",
    "data['sender_name_embedding'] = data['sender_name_embedding'].apply(lambda x: np.array(x, dtype=np.float32))\n",
    "\n",
    "# Expand arrays into separate columns\n",
    "message_embedding_data = pd.DataFrame(data['message_embedding'].tolist(), index=data.index).add_prefix('message_embedding_')\n",
    "recipient_name_embedding_data = pd.DataFrame(data['recipient_name_embedding'].tolist(), index=data.index).add_prefix('recipient_name_embedding_')\n",
    "sender_name_embedding_data = pd.DataFrame(data['sender_name_embedding'].tolist(), index=data.index).add_prefix('sender_name_embedding_')\n",
    "\n",
    "# Concatenate embeddings into the original dataframe\n",
    "data = pd.concat([data, message_embedding_data, recipient_name_embedding_data, sender_name_embedding_data], axis=1)\n",
    "\n",
    "# Drop the original embedding columns\n",
    "data = data.drop(columns=['message_embedding', 'recipient_name_embedding', 'sender_name_embedding'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6fe53454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Columns: 307 entries, transaction_amount to sender_name_embedding_99\n",
      "dtypes: datetime64[ns](1), float32(300), float64(2), object(4)\n",
      "memory usage: 12.0+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ff6f7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Columns: 302 entries, transaction_amount to sender_name_embedding_99\n",
      "dtypes: float32(300), float64(2)\n",
      "memory usage: 11.6 MB\n"
     ]
    }
   ],
   "source": [
    "# Identify columns with object and datetime64[ns] data types\n",
    "columns_to_drop = data.select_dtypes(include=['object', 'datetime64[ns]']).columns\n",
    "\n",
    "# Drop the identified columns\n",
    "data = data.drop(columns=columns_to_drop)\n",
    "\n",
    "data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d5d1f2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['is_scam'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report, accuracy_score\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Prepare features and target variable\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mis_scam\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m y \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_scam\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Split the data into training and testing sets\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:5581\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5434\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5435\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5442\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5443\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5445\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5446\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5579\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5583\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5587\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5588\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5589\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:4788\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4788\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:4830\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4828\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4829\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4830\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4831\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4833\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4834\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['is_scam'] not found in axis\""
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Prepare features and target variable\n",
    "X = data.drop(columns=['is_scam'])\n",
    "y = data['is_scam']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Classification Report:\\n{report}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde8acb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
